---
title: "lab1_assignment3"
author: "Martynas Lukosevicius, Alejo Perez Gomez, Shwetha Vandagadde Chandramouly"
date: "11/7/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment 2

## 1.

Bayesian Model:  
$\hat y \sim N(w_0 +w^tX, \sigma^2)$ where $w \sim N(0, \frac{\sigma^2}{\lambda})$

* $w$ - weights
* $X$ - features
* $\lambda$ - regularization penalty
* $\sigma$ - standard deviation

<!-- Bayes' theorem: -->

<!-- $$ p(w|d)\propto(D|w)p(w)$$ -->
<!-- Probability of y: -->

<!-- $$ p(y| X, w,\sigma) = N(w_0 +w^tX_i, \sigma^2) $$ -->

<!-- Prior probability: -->

<!-- $$ p(w) \sim N(0, \frac{\sigma^2}{\lambda}) = \frac 1 {\sqrt{2 \pi \frac {\sigma^2 }{\lambda}} } e^ - { \frac {(w)^2}{2 \frac {\sigma^2}{\lambda}}} $$ -->
<!-- Likelihood: -->

<!-- $$ p(D|w) = \prod^n_{i = 1} N(w_0 +w^tX_i, \sigma^2) $$ -->

<!-- $$ p(D|w) = \prod^n_{i = 1} \frac 1 {\sqrt{2 \pi \sigma^2} } e^ - { \frac {(y - X_iw)^2}{2 \sigma^2}} = \frac 1 {(\sqrt{2 \pi \sigma^2})^n } e^ - \sum^n_{i = 1} \frac{(y_i - w^TX_i)^2} {2\sigma^2} $$ -->

<!-- Model: -->

<!-- $$ p(w|D) \propto \frac 1 {(\sqrt{2 \pi \sigma^2})^n } e^ - \sum^n_{i = 1} \frac{(y_i - w^TX_i)^2} {2\sigma^2}  * \frac 1 {\sqrt{2 \pi \frac {\sigma^2 }{\lambda}} } e^ - { \frac {(w)^2}{2 \frac {\sigma^2}{\lambda}}} = \frac{\sqrt \lambda}{(\sqrt{2 \pi \sigma^2})^{n+1}} e^{- \frac{\sum^n_{i=1} (y_i - w^TX_i)^2 + w^2\lambda}{2\sigma^2}} $$ -->

## 2.

Scaling data:

```{r, message = FALSE}
library(readr)
parkinsons <- read_csv("parkinsons.csv")
cleaned <- parkinsons[c(-1:-4, -6)]
parkinsons.scaled <- scale(cleaned)
set.seed(12345)
n <- dim(parkinsons.scaled)[1]
id=sample(1:n, floor(n*0.6))
train=parkinsons.scaled[id,]
test=parkinsons.scaled[-id,]
```


## 3.

Check Lecture 1d block 1 slide 16

As we will be optimizing $\sigma$ and $w$, likelihood and prior should contain all $\sigma$, even if data is scaled (so it means that $\sigma \sim 1$): 
 
$$ log(posterior) = log(likelihood* prior) = log(likelihood) + log(prior)$$

### a)

loglikelihood:
$$log (p(D|w)) = - \frac n {2} log(2 \pi \sigma^2) - \sum^n_{i = 1} \frac{(y_i - w^TX_i)^2} {2\sigma^2}$$

```{r}

loglikelihood <- function(w, sigma){
n <- dim(train)[1]
part1 <- -(n/ 2) * log(2 * pi*(sigma^2))

y <- train[, 1]
x <- train[, -1]
res <- sum((y - (x %*% w))^2)

return(part1 - (res/(2*(sigma^2))))
}

```

### b)

Ridge part $\sim$ log prior, where $\tau = { \frac {\sigma^2} \lambda},$ and $p$ number of weights:


$$ log(prior) =  -{p \over 2}log({2 \pi \tau}) - { \frac {\sum^p(w_i)^2}{2 \tau}} $$

function returns $- log(posterior)$

```{r}

ridge <- function(x, lambda){
  w <- x[1:16]
  sigma <- x[17]
  p <- length(w)
  tau <- sigma^2 / lambda
  part1 <-  (-1/2) * log(2* pi * tau)
  part2 <- (w ^ 2) / (2* tau)
  ridge <- part1 - part2
  # ridge <- lambda * (w %*% w)
  return( -(loglikelihood(w,sigma) + sum(ridge)))
  #return(sum(ridge))
}

```

### c)

function to predict weights ($w$) and $\sigma$
```{r}
ridgeOpt <- function(lambda){
  x <- c(rep(1,16),1)
  a <- optim(x ,ridge, method = "BFGS", lambda = lambda)
  w <- a$par[1:16]
  sigma <- a$par[17]
  return(a)
}

```

### d)

function to calculate degrees of freedom
```{r}

DF <- function(lambda){
  m <- as.matrix(train[ ,-1])
  part1 <- t(m) %*% m + (lambda * diag(16))
  part2 <- m %*% solve(part1) %*% t(m)
  return(sum(diag(part2)))
}


```

## 4.

```{r , echo=FALSE, results = FALSE}
task4 <- function(lambda){
  a <- ridgeOpt(lambda)
  n <- dim(train)[1]
  
  w <- a$par[1:16]
  
  predict <- train[ , -1] %*% w
  y <- train[ ,1]
  
  MSEtrain <- (1/n) * t((y-predict)) %*% (y-predict)
  n <- dim(test)[1]
  predict <- test[ , -1] %*% w
  y <- test[ ,1]
  
  MSEtest <- (1/n) * t((y-predict)) %*% (y-predict)
  result <- list(w = w, sigma = a$par[17], testMSE = MSEtest, trainMSE = MSEtrain)
  return(result)

}
```

```{r , echo= FALSE}

lambda1 <- task4(1)
lambda2 <- task4(100)
lambda3 <- task4(1000)
# nn <- rbind(c(lambda1$w, lambda1$sigma), c(lambda2$w, lambda2$sigma), c(lambda3$w, lambda3$sigma))
# row.names(nn) <- c("lambda 1", "lambda 2", "lambda 3")
# col_names <- c(sprintf("w%d", 1:16), "sigma")
# colnames(nn) <- col_names
mm <- rbind(c(lambda1$trainMSE, lambda1$testMSE),
            c(lambda2$trainMSE, lambda2$testMSE),
            c(lambda3$trainMSE, lambda3$testMSE))
row.names(mm) <- c("lambda = 1", "lambda = 100", "lambda = 1000")
colnames(mm) <- c("MSE train", "MSE test")
knitr::kable(mm)
```
 $\lambda = 100$ is better than others because MSE for train set and for test set is lowest. MSE is good loss function because it comes from model's MLE.

## 5.

```{r, echo=FALSE, results = FALSE}

DF <- function(lambda){
  m <- as.matrix(parkinsons.scaled[ ,-1])
  part1 <- t(m) %*% m + (lambda * diag(16))
  part2 <- m %*% solve(part1) %*% t(m)
  return(sum(diag(part2)))
}

loglikelihood <- function(w, sigma){
n <- dim(train)[1]
part1 <- -(n/ 2) * log(2 * pi*(sigma^2))

y <- train[, 1]
x <- train[, -1]
res <- sum((y - (x %*% w))^2)

return(part1 - (res/(2*(sigma^2))))
}

```

```{r , echo=FALSE }
AIC1 <- -2 * loglikelihood(lambda1$w, lambda1$sigma) + 2 * DF(1)
AIC2 <- -2 * loglikelihood(lambda2$w, lambda2$sigma) + 2 * DF(100)
AIC3 <- -2 * loglikelihood(lambda3$w, lambda3$sigma) + 2 * DF(1000)
#print(paste("AIC1:", AIC1, "AIC2:", AIC2, "AIC3:", AIC3))

AIC <- c(AIC1 , AIC2 , AIC3)
names(AIC) <- c("lambda = 1", "lambda = 100", "lambda = 1000")
knitr::kable(as.matrix(AIC, nrow= 1), row.names = TRUE,  col.names = c("AIC") )
res <- which.min(AIC)
res <- ifelse(res == 1, "1", ifelse(res == 2, 100, 1000))
```

The optimal model is with lowest AIC score, in this case its a model with $\lambda =$  `r res`. Hold out method requires to divide data into 3 parts: train,validation, test, which wont allow to use all data for training, its not the case with AIC, where its use on training + validation data

However im not sure about it.... 



# Assignment 3

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(glmnet)
data = read.csv("tecator.csv", header = TRUE)
n = nrow(data)
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
```

**1. **

Here we are assuming that fat can be modeled as linear regression with channels as features.

Underlying probalistic model is :--- correction : see ols model in slide 1d
$$\hat{y} = \beta_0 +\sum_{i = 1}^{100} \beta_i*x_i + \epsilon ,\\\epsilon \sim {\sf N}(0, \sigma^2)$$
*$y$ = dependent variable 
*$x_i$ = features (channel1 to channel100)
*$\beta_0$ = y-intercept
*$\beta_i$ = slope coefficient for each feature
$\epsilon$ = model error term

**Fitting linear regression model to training data**

```{r}
X_train = as.matrix(train[2:101])
Y_train = as.matrix(train$Fat)
fit_train = lm(Fat ~ . , data = train[2:102]) #fitting linear regression to training data
y_hat_train = predict(fit_train)
```
Error for training and test data :

```{r,echo=FALSE}
train_mse = mean((y_hat_train-Y_train)^2) 
cat("MSE for training data is ",train_mse)
cat("MAE for training data is ",mean(abs(y_hat_train - Y_train)))
#scatter.smooth(Y_train,y_hat_train)
new_x=as.matrix((test[, 2:101]))
Y_test = as.matrix((test[,102]))
y_hat_test = predict(fit_train, newdata = as.data.frame(new_x))
#scatter.smooth(Y_test,y_hat_test)
test_mse = mean((y_hat_test-Y_test)^2) 
cat("\nMSE for testing data ",test_mse)
cat("MAE for training data is ",mean(abs(y_hat_test - Y_test)))
```

We can see that error(MSE and MAE) for the train data was low , but when we try to fit the model to test data error increases. This is because of the overfitting on training data due to large number of features in our model.
Quality of fit : Overfit.
Prediction quality is not good for test data , this is evident by observing increase in MSE and MAE in test data.

Over all quality of the model is not good , as there is large number of features here , regularization is required.

**2. **

In this case as the number of input features are high, high degree of polynomials lead to overfitting.

In LASSO , we try to choose the value of $\beta$ such that the following loss function is minimized
$$\sum_{i = 1}^{n}\left(Y_i-\beta_0-\sum_{j = 1}^{p}\beta_jX_{ji}\right)^2+\lambda\sum_{j = 1}^{p}|\beta_j|$$


**3. **

Fitting LASSO regression model to training data

```{r}
covariates = as.matrix(train[ ,2:101])
response = as.matrix(train[ ,102])
model_lasso = glmnet((covariates), response, alpha = 1, family = "gaussian")
plot(model_lasso, xvar = "lambda", label = T)
```

To choose lambda that has only 3 features , we can perform cross validation and check for lambda values which has only 3 variables. 

```{r,echo=FALSE}
model=cv.glmnet(as.matrix(covariates),
response, alpha=1,family="gaussian")
model_lasso_3 = glmnet(as.matrix(covariates), response, alpha = 1, family = "gaussian", lambda = model$lambda[which(model$nzero == 3)])
plot(model_lasso_3, xvar = "lambda", label = T)
cat("Lambda values that give 3 variables are :",model$lambda[which(model$nzero == 3)])
```
**4. **
Dependence of degrees of freedom on panalty factor

```{r, echo=FALSE}
df = function(lambda){
ld = lambda * diag(ncol(covariates))
H = covariates %*% solve(t(covariates) %*% covariates + ld) %*% t(covariates)
DOF = sum(diag(H))
return(DOF)
}
lambda_values = model_lasso$lambda
degrees_of_freedom = c()
for(i in lambda_values){
degrees_of_freedom = cbind(degrees_of_freedom,df(i))
}
plot(lambda_values, degrees_of_freedom)
```
Here as expected we can see that degrees of freedom decreases as penalty factor increases. When lambda is 0 , df is infinity and when df is 0, lambda is infinity.

**5. **
Fitting ridge model

```{r,echo = FALSE}
covariates = as.matrix(train[ ,2:101])
response = as.matrix(train[ ,102])
model_ridge = glmnet((covariates), response, alpha = 0, family = "gaussian")
plot(model_ridge, main = "RIDGE", xvar = "lambda", label = T)
```

In both lasso and ridge we can see that the value of coefficents decreases with increase in lambda values, But only in case of lasso , coeffients converge to 0 with increase in lambda. 

**6. **
To find optimal lambda, we use cross validation.

```{r,echo=FALSE}
model_l=cv.glmnet((covariates), response, alpha=1,family="gaussian")
best_lambda = model_l$lambda.min
plot(model_l)
plot(model_l$lambda,model_l$cvm,xlab="lambda",ylab="Mean CV error")
cat("Optimal lambda is :",best_lambda)
```
Relationship between log(lambda) and MSE:
As the lambda increases, the value of log(lambda) also increases, and we can see from the plot above that the MSE also increases with increasing lambda.
Best lambda is given as zero for the above model , this makes sense as we can see from cvm(mean cross-validated error) vrs lambda plot , the error is minimum when lambda is zero.

Fitting the model for 0 and log_lambda = -2 for comparision.

```{r}
m = cv.glmnet((covariates), response, alpha=1,family="gaussian",lambda=c(best_lambda,0.1353353))
```
```{r,echo = FALSE}
cat("CV Error when lambda is ",m$lambda[1] ," =",m$cvm[1])
cat(" and CV Error when lambda is ",m$lambda[2] ," =",m$cvm[2])
```
Log(bestlambda) ie log(0) is -Inf, from the plot of MSE vrs log(lambda) we can see that MSE is lowest for log(lambda) = -7(in the plot this is the lowest negetive number) for when compared to log(lambda) = -2.
Also in lambda vrs Mean Cv error plot , we can see that for lambda = 0.1353353 (log(lambda)=-2 , so lambda = exp(-2) = 0.1353353) error is higher than error for lambda = 0.

Plot for Y vrs predicted Y for test data using optimal lambda

```{r,echo = FALSE}
model_optimallasso =  glmnet((covariates), response, alpha = 1, family = "gaussian", lambda = best_lambda)
y = as.matrix((test[,102]))
ypred=predict(model_optimallasso, newx=as.matrix((test[, 2:101])), type="response")
scatter.smooth(y,ypred)
cat("coefficient of determination is ",
sum((ypred-mean(y))^2)/sum((y-mean(y))^2), "and MSE is ", mean((y-ypred)^2))

```

From the scatterplot we can see that y is similar to predited y and also the value of coefficient of determination is very close to 1 , this shows that lasso model is pretty good at predicting the Y for test data.

**7. **

Generating response using test data as features and optimal Lasso model

```{r,echo=FALSE}
betas = as.vector((as.matrix(coef(model_optimallasso))[-1, ])) # removing the first row for intercept
resid = response - (covariates %*% betas)
sigma = sd(resid)

ypred=predict(model_optimallasso, newx=as.matrix((test[, 2:101])), type="response")
set.seed(90000)
y_gen = rnorm(108,ypred,sigma)
scatter.smooth(y,y_gen)
cat("coefficient of determination is ",sum((y_gen-mean(y))^2)/sum((y-mean(y))^2)," MSE is ",mean((y_gen - y)^2))

```
The generated data seem to fit well as we can see value of coefficient of determination is close to 1 and MSE is also low.
